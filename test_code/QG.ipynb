{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "/opt/anaconda3/envs/NLP/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:973: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "# Tip: By now, install transformers from source\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import pandas as pd \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question(answer, context, max_length=64):\n",
    "    input_text = \"answer: %s  context: %s </s>\" % (answer, context)\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    output = model.generate(input_ids=features['input_ids'], \n",
    "                  attention_mask=features['attention_mask'],\n",
    "                  max_length=max_length)\n",
    "\n",
    "    return tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_generation(context):\n",
    "    print(\"context: \" + context)\n",
    "    answer = input(\"answer: \")\n",
    "    print(answer)\n",
    "    question = get_question(answer, context)[16:-4]\n",
    "    print(\"question: \" + question)\n",
    "    add_question = input(\"add some word in questions : \")\n",
    "    difficulty = input(\"difficulty(e,m,h,f): \")\n",
    "    print(difficulty + '\\n')\n",
    "\n",
    "    return answer, question + '' + add_question, difficulty.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_extract(dir):\n",
    "    document = [] \n",
    "    with open(\"documents\"+dir, \"r\") as file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if line != '\\n' and line[0] != '=':\n",
    "                sentence = line.strip('\\n').split(\". \")\n",
    "                document.extend(sentence)\n",
    "    \n",
    "    return document\n",
    "\n",
    "def sentence_extract2(dir):\n",
    "    document = [] \n",
    "    with open(\"documents\"+dir, \"r\") as file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if line != '\\n' and line[0] != '=':\n",
    "                sentence = line.strip('\\n').split(\". \")\n",
    "                document.extend(sentence)\n",
    "\n",
    "    two_sentence = ''\n",
    "    new_document = []\n",
    "    for i in document:\n",
    "        if two_sentence == '':\n",
    "            two_sentence = i\n",
    "        else:\n",
    "            new_document.append(two_sentence + '. ' + i + '.')\n",
    "            two_sentence = ''\n",
    "    \n",
    "    return new_document\n",
    "\n",
    "def sentence_extract3(dir):\n",
    "    document = [] \n",
    "    with open(\"documents\"+dir, \"r\") as file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if line != '\\n' and line[0] != '=':\n",
    "                sentence = line.strip('\\n').split(\". \")\n",
    "                document.extend(sentence)\n",
    "\n",
    "    num_sentence = 0\n",
    "    sentence = ''\n",
    "    new_document = []\n",
    "    for i in document:\n",
    "        if num_sentence == 0:\n",
    "            sentence = i\n",
    "            num_sentence += 1\n",
    "        elif num_sentence < 2:\n",
    "            sentence = sentence + '. ' + i\n",
    "            num_sentence += 1\n",
    "        else:\n",
    "            new_document.append(sentence + '. ' + i)\n",
    "            sentence = ''\n",
    "            num_sentence = 0\n",
    "    \n",
    "    return new_document\n",
    "dir = \"/languages/Mandarin_language.txt\"\n",
    "document = sentence_extract3(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: Knowledge of this language was thus essential for an official career, but it was never formally defined.Officials varied widely in their pronunciation; in 1728, the Yongzheng Emperor, unable to understand the accents of officials from Guangdong and Fujian, issued a decree requiring the governors of those provinces to provide for the teaching of proper pronunciation. Although the resulting Academies for Correct Pronunciation (正音書院; Zhèngyīn Shūyuàn) were short-lived, the decree did spawn a number of textbooks that give some insight into the ideal pronunciation. Common features included:\n",
      "no\n",
      "question: Was pronunciation formally defined?\n",
      "h\n",
      "\n",
      "context: loss of the Middle Chinese voiced initials except for v-. merger of -m finals with -n. the characteristic Mandarin four-tone system in open syllables, but retaining a final glottal stop in \"entering tone\" syllables\n",
      "no\n",
      "question: Does the Mandarin four-tone system have a final glottal stop?\n",
      "f\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "questions = []\n",
    "documents = []\n",
    "difficulties = []\n",
    "for i, sentence in enumerate(document):\n",
    "    if i < 20:\n",
    "        continue\n",
    "    elif i == 22:\n",
    "        break\n",
    "    a, q, d = qa_generation(sentence)\n",
    "\n",
    "    if d =='F':\n",
    "        continue\n",
    "        \n",
    "    answers.append(a)\n",
    "    questions.append(q)\n",
    "    difficulties.append(d) # E, M, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'document_id': dir, 'question': questions, 'answer': answers, 'difficulty': difficulties}  \n",
    "df = pd.DataFrame(dict) \n",
    "df.to_csv('test.csv', index=False)  \n",
    "#df.to_csv('test.tsv', index=False, sep=\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv ('result_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    40\n",
       "E    30\n",
       "H    30\n",
       "Name: difficulty, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['difficulty'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60951fe2ccac1c258907b209f658f6ef884c93d9ccb3f36667a6035df696e51a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
